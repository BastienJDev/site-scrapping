# üöÄ D√©ploiement via GitHub - Guide pas √† pas

## ‚úÖ √âtape 1: Cr√©er un repository sur GitHub

1. Allez sur https://github.com
2. Cliquez sur le bouton **"+"** en haut √† droite ‚Üí **"New repository"**
3. Remplissez:
   - **Repository name:** `site-scrapping` (ou le nom de votre choix)
   - **Description:** "Dashboard de scraping web avec Gemini AI"
   - **Visibilit√©:** Private (recommand√©) ou Public
   - ‚ö†Ô∏è **NE PAS** cocher "Add README" (on a d√©j√† fait le commit)
4. Cliquez sur **"Create repository"**

## ‚úÖ √âtape 2: Lier votre projet local √† GitHub

GitHub va vous montrer des commandes. Utilisez celles-ci:

```bash
cd "/Users/bastienjund/Desktop/Site Scrapping"

# Ajouter le remote (remplacer par VOTRE URL)
git remote add origin https://github.com/VOTRE_USERNAME/site-scrapping.git

# V√©rifier
git remote -v

# Pousser le code
git branch -M main
git push -u origin main
```

**Note:** Remplacez `VOTRE_USERNAME` par votre nom d'utilisateur GitHub

## ‚úÖ √âtape 3: Sur votre VPS - Cloner le projet

```bash
# Se connecter au VPS
ssh user@IP_VPS

# Installer Git si n√©cessaire
sudo apt update
sudo apt install git -y

# Cloner le projet
cd /home/user
git clone https://github.com/VOTRE_USERNAME/site-scrapping.git
cd site-scrapping
```

## ‚úÖ √âtape 4: Configuration sur le VPS

```bash
# Installer Python 3.10+
sudo apt install python3.10 python3.10-venv python3-pip -y

# Cr√©er environnement virtuel
python3.10 -m venv venv
source venv/bin/activate

# Installer d√©pendances
pip install --upgrade pip
pip install -r requirements.txt

# Cr√©er le fichier .env (IMPORTANT!)
nano .env
```

Dans `.env`, ajoutez:
```
GEMINI_API_KEY=VOTRE_CLE_API_ICI
```

Sauvegarder avec `Ctrl+O`, `Enter`, `Ctrl+X`

```bash
# Cr√©er data.json
echo '{"categories": [], "sites": [], "scraping_results": []}' > data.json

# Tester
python3 app.py
```

Si √ßa fonctionne (Ctrl+C pour arr√™ter), passez √† l'√©tape 5.

## ‚úÖ √âtape 5: Configuration Production (Gunicorn + Systemd)

```bash
# Installer Gunicorn
source venv/bin/activate
pip install gunicorn

# Cr√©er le service systemd
sudo nano /etc/systemd/system/scrapping.service
```

Contenu du fichier:
```ini
[Unit]
Description=Site Scrapping Dashboard
After=network.target

[Service]
User=VOTRE_USER
WorkingDirectory=/home/VOTRE_USER/site-scrapping
Environment="PATH=/home/VOTRE_USER/site-scrapping/venv/bin"
ExecStart=/home/VOTRE_USER/site-scrapping/venv/bin/gunicorn -w 4 -b 0.0.0.0:5001 app:app --timeout 120
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
```

‚ö†Ô∏è Remplacez `VOTRE_USER` par votre nom d'utilisateur

```bash
# Activer et d√©marrer le service
sudo systemctl daemon-reload
sudo systemctl enable scrapping
sudo systemctl start scrapping
sudo systemctl status scrapping
```

## ‚úÖ √âtape 6: Configuration Firewall

```bash
# Ouvrir le port 5001
sudo ufw allow 5001/tcp

# Ou si vous utilisez Nginx (recommand√©):
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
```

## üîÑ Mises √† jour futures

### Sur votre Mac (apr√®s modifications):
```bash
cd "/Users/bastienjund/Desktop/Site Scrapping"
git add .
git commit -m "Description des changements"
git push
```

### Sur votre VPS:
```bash
cd /home/user/site-scrapping
git pull
source venv/bin/activate
pip install -r requirements.txt  # Si nouvelles d√©pendances
sudo systemctl restart scrapping
```

## üåê Bonus: Nginx + SSL (Production)

### Installer Nginx
```bash
sudo apt install nginx -y
sudo nano /etc/nginx/sites-available/scrapping
```

Contenu:
```nginx
server {
    listen 80;
    server_name votre-domaine.com;  # ou votre IP

    location / {
        proxy_pass http://127.0.0.1:5001;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_read_timeout 120s;
    }

    location /static {
        alias /home/VOTRE_USER/site-scrapping/static;
        expires 30d;
    }
}
```

```bash
# Activer le site
sudo ln -s /etc/nginx/sites-available/scrapping /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx

# Modifier le service pour √©couter seulement en local
sudo nano /etc/systemd/system/scrapping.service
# Changer: -b 0.0.0.0:5001 ‚Üí -b 127.0.0.1:5001
sudo systemctl daemon-reload
sudo systemctl restart scrapping
```

### SSL avec Let's Encrypt
```bash
sudo apt install certbot python3-certbot-nginx -y
sudo certbot --nginx -d votre-domaine.com
```

## üìä Commandes utiles

```bash
# Voir les logs en temps r√©el
sudo journalctl -u scrapping -f

# Red√©marrer l'application
sudo systemctl restart scrapping

# Arr√™ter l'application
sudo systemctl stop scrapping

# Statut
sudo systemctl status scrapping

# Voir les derni√®res erreurs
sudo journalctl -u scrapping -n 50 --no-pager
```

## üîê S√©curit√© - Checklist finale

- [ ] `.env` non commit√© dans Git (v√©rifi√© par `.gitignore`)
- [ ] Mots de passe chang√©s dans `app.py`
- [ ] Mode debug d√©sactiv√© dans `app.py` (mettre `debug=False`)
- [ ] Firewall configur√©
- [ ] SSL activ√© (si domaine)
- [ ] Backup r√©gulier de `data.json`

## ‚ö†Ô∏è IMPORTANT avant le premier push

Avant de faire `git push`, v√©rifiez que `.env` n'est PAS dans Git:
```bash
git status
# Si vous voyez .env, faites:
git rm --cached .env
echo ".env" >> .gitignore
git add .gitignore
git commit -m "Remove .env from tracking"
```

## üÜò D√©pannage

### Erreur "Permission denied (publickey)"
Configurez une cl√© SSH ou utilisez HTTPS avec token:
```bash
git remote set-url origin https://VOTRE_TOKEN@github.com/VOTRE_USERNAME/site-scrapping.git
```

### Le service ne d√©marre pas
```bash
sudo journalctl -u scrapping -n 50
# V√©rifier les chemins dans le fichier service
```

### Port 5001 d√©j√† utilis√©
```bash
sudo lsof -i :5001
sudo kill -9 PID
```
